# Logistic-Regression Mini-Project ðŸ“ˆ  

**Author:** TuÄŸcan TopaloÄŸlu  

---

## 1. Project Overview
This miniâ€‘project implements **Logistic Regression** from scratch to perform binary classification.  
To boost generalisation and prevent overâ€‘fitting, the model supports:

| Technique | Purpose |
|-----------|---------|
| **L2 regularisation** | Weight shrinkage |
| **Early stopping** | Halt training when validation loss stops improving |
| **Threshold optimisation** | Tune the decision boundary for best F1 / balanced accuracy |

All training, evaluation and visualisation steps are reproducible with a single command.

---

## 2. Repository Layout

```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ hw1Data.txt            # raw data
â”‚   â”œâ”€â”€ train_data.txt         # train split
â”‚   â”œâ”€â”€ validate_data.txt      # validation split
â”‚   â””â”€â”€ test_data.txt          # test split
â”œâ”€â”€ results/                   # created automatically
â”‚   â”œâ”€â”€ epoch_loss_output.txt  # per-epoch loss
â”‚   â”œâ”€â”€ weights.txt            # learned weights
â”‚   â”œâ”€â”€ scores.txt             # accuracy / precision / recall / F1
â”‚   â”œâ”€â”€ *.png                  # data & loss plots (before/after optimisation)
â”œâ”€â”€ main.py                    # entry point
â”œâ”€â”€ DataHandler.py             # I/O, splitting, plotting helpers
â”œâ”€â”€ LogisticRegression.py      # the core algorithm
â””â”€â”€ README.md
```

---

## 3. Algorithms & Techniques

1. **Logistic Regression** â€“ gradientâ€‘descent implementation  
2. **L2 Regularisation** â€“ prevents large weights  
3. **Early Stopping** â€“ stops when validation loss fails to drop for _n_ epochs  
4. **Threshold Search** â€“ picks the cutâ€‘off that maximises F1 (optional)

---

## 4. Getting Started

### 4.1 Requirements

| Package | Tested version |
|---------|----------------|
| NumPy | 2.1.3 |
| Pandas | 2.2.3 |
| Matplotlib | 3.9.2 |
| scikit-learn | 1.5.2 |
| Python | 3.8â€¯+ (tested on 3.10 & 3.12) |

Install them in one go:

```bash
pip install -r requirements.txt
```

### 4.2 Run

```bash
python main.py
```

* If the dataset is not yet split, the script performs a **70 / 15 / 15** split automatically and writes the files to `./data`.  
* All output artefacts are placed in `./results`.

---

## 5. Inspecting Results

| File | Description |
|------|-------------|
| `results/scores.txt` | Overall metrics (Accuracy, Precision, Recall, F1). Appended if the file exists. |
| `results/epoch_loss_output.txt` | Training / validation loss per epoch. |
| `results/*Before_*.png` | Loss curves before optimisation. |
| `results/*After_*.png` | Loss curves after earlyâ€‘stopping + threshold tuning. |
| `results/Data_Plot.png` | Scatter plot of the original data. |

Open the PNG files to visually compare optimisation effects.

---

## 6. Notes & Tips

* Result files are created automatically if absent; otherwise the script appends new runs.  
* Keep datasets under `./data` with the original column order.  
* The project works with newer library versions, but the tested ones are listed for reproducibility.  
* If you encounter an issue, please open an issue â€” contributions welcome!

Happy experimenting! ðŸš€
